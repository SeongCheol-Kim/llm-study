{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# initialize openai\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = Image.open('../data/room-dataset/living/living_200.jpg')\n",
    "# sample_image ## 거실 이미지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- search의 종류\n",
    "    - step 1 (유사한 분위기를 연출한 거실 이미지 탐색)\n",
    "        - image to image\n",
    "        - text to image\n",
    "            - caption, gpt-4v description, etc\n",
    "    - step 2 (이미지 내에 있는 물건들을 활용)\n",
    "        - 각 가구들끼리의 이미지 유사도 측정 (img-emb-sim 측정)\n",
    "    - (추가) filtering (meta data)\n",
    "\n",
    "- 이미지에서 정보를 최대한 많이 추출하여 데이터 포인트로 생성\n",
    "    - 이미지의 전반적인 분위기 (image embeddings)\n",
    "    - 이미지에 대한 설명 (image description using GPT-4V)\n",
    "    - 이미지 내에 있는 가구들 (object detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- 이미지 설명 생성 (description generation)\n",
    "- object detection (words)\n",
    "- 각 이미지의 좌표 위치 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. GPT-4V를 활용하여 다양한 정보 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rate limit을 고려하여 GPT-4V api call\n",
    "\n",
    "참고 : https://platform.openai.com/docs/guides/vision\n",
    "\n",
    "Rate limit 확인 : https://platform.openai.com/account/limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT-4v를 활용하여 이미지 설명 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompt = \"\"\"Please analyze the living room image provided.  \n",
    "Include 'Color Scheme', 'Lighting', 'Spatial Layout', and 'Architectural Features' with descriptions based on the room's characteristics.\n",
    "The output should be formatted in a JSON-like dictionary structure. Each image should be done separately.\n",
    "\n",
    "Example output :\n",
    "\n",
    "```json\n",
    "  {\n",
    "    \"Color Scheme\": <Description about color scheme>,\n",
    "    \"Lighting\": <Description about lighting>,\n",
    "    \"Spatial Layout\": <Description about spatial layouts >,\n",
    "    \"Architectural Features\": <Descrption about architectural features>\n",
    "  }\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "headers = {\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"Authorization\": f\"Bearer {openai.api_key}\"\n",
    "}\n",
    "\n",
    "img = encode_image('../data/room-dataset/living/living_18.jpg')\n",
    "img2 = encode_image('../data/room-dataset/living/living_5.jpg')\n",
    "\n",
    "payload = {\n",
    "  \"model\": \"gpt-4-vision-preview\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": text_prompt\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{img}\"\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{img2}\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "  ],\n",
    "  \"max_tokens\": 1000\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tier2 기준\n",
    "\n",
    "# TPM = 20000\n",
    "# RPM = 100\n",
    "# RPD = 1000\n",
    "\n",
    "# Tier1 기준\n",
    "\n",
    "TPM = 10000\n",
    "RPM = 100\n",
    "RPD = 1000\n",
    "\n",
    "low_res = True # 저화질 : 512x512 사이즈 이미지를 Input으로\n",
    "\n",
    "if low_res:\n",
    "    token_per_img = 65\n",
    "    text_token = 115\n",
    "    print(\"1분에 최대 {}번의 api call 가능.\".format( min(TPM//(token_per_img*2+text_token), RPM)))\n",
    "else:\n",
    "    print(\"기본 65 토큰 + 512px 사이즈로 crop 된 이미지 개수 x 129 토큰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = response.json()['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gpt-4v는 json 형태로 결과를 내어줄 수 없다\n",
    "    - 따라서 전반적인 image descprition을 받은 후 검증이 필요함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT-4v를 활용하여 이미지 설명 생성 (이미지 100개에 적용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_image(input_prompt, image_paths, openai_key):\n",
    "  headers = {\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {openai_key}\"\n",
    "      }\n",
    "  imgs = [encode_image(i) for i in image_paths]\n",
    "\n",
    "  payload = {\n",
    "          \"model\": \"gpt-4-vision-preview\",\n",
    "          \"messages\": [{\"role\": \"user\",\n",
    "                      \"content\": []\n",
    "                      },\n",
    "                      ],\n",
    "          \"max_tokens\": 1000\n",
    "          }\n",
    "  \n",
    "  img_contents = [{\"type\": \"text\", \"text\": input_prompt}]\n",
    "  for img in imgs:\n",
    "    input_template = {\n",
    "      \"type\": \"image_url\",\n",
    "      \"image_url\": {\n",
    "        \"url\": f\"data:image/jpeg;base64,{img}\"\n",
    "      }\n",
    "    }\n",
    "    img_contents.append(input_template)\n",
    "\n",
    "  payload['messages'][0]['content'] = img_contents\n",
    "\n",
    "  response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "  output = response.json()['choices'][0]['message']['content']\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1번부터 100번 이미지까지, 2개씩\n",
    "img_paths = list(os.walk('../data/room-dataset/living'))[0][2]\n",
    "img_paths = [i for i in img_paths if i!=\".DS_Store\"]\n",
    "img_paths = [i for i in img_paths if int(i.split('_')[1].split('.')[0]) in list(range(1, 101))]\n",
    "\n",
    "def extract_number(filename):\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    return int(match.group()) if match else 0\n",
    "\n",
    "img_paths = sorted(img_paths, key=extract_number)\n",
    "img_paths = [os.path.join('../data/room-dataset/living', i) for i in img_paths]\n",
    "\n",
    "batches = [img_paths[i : i+2] for i in range(0, len(img_paths), 2)]\n",
    "outputs = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT-4V는 JSON 형태의 아웃풋을 강제할 수 없음\n",
    "- 단순 이미지 description을 활용하여 유사도 측정을 해도 무방함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 과금 주의\n",
    "for batch in tqdm(batches):\n",
    "    r = describe_image(text_prompt, batch, openai.api_key)\n",
    "    batch1 = batch[0].split('/')[-1]\n",
    "    batch2 = batch[1].split('/')[-1]\n",
    "    outputs[batch1 + \"#\" + batch2] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../data/room-dataset/room_descriptions.json\", 'w') as file:\n",
    "#     json.dump(outputs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/room-dataset/room_descriptions.json\", 'r') as file:\n",
    "    outputs = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs['living_1.jpg#living_2.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs['living_11.jpg#living_12.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(text):\n",
    "    matches = re.findall(r'(\\{[\\s\\S]*?\\})', text)\n",
    "    matches = [json.loads(m) for m in matches]\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs['living_11.jpg#living_12.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_response(outputs['living_11.jpg#living_12.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = list()\n",
    "\n",
    "for k,v in outputs.items():\n",
    "    try:\n",
    "        parsed = parse_response(v)\n",
    "        if len(parsed)<2:\n",
    "            failed.append(k)\n",
    "        else:\n",
    "            outputs[k] = parsed    \n",
    "    except:\n",
    "        failed.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict 안에 dict가 있는 등 일관된 결과를 얻기 힘들다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs['living_91.jpg#living_92.jpg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chat_completion api를 활용하여 비정형 text를 json format으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_formatting_prompt = \"\"\"Using the provided text, find the smallest format of json there is and store them in a list as separate elements.\n",
    "The ouput list should have two json objects found from the provided text.\n",
    "\n",
    "Desired output :\n",
    "{'list': [{'Image 1': {'Color Scheme': <Color Scheme>,\n",
    "    'Lighting': <Lighting>,\n",
    "    'Spatial Layout': <Spatial Layout>,\n",
    "    'Architectural Features': <Architectural Features>}},\n",
    "  {'Image 2': {'Color Scheme': <Color Scheme>,\n",
    "    'Lighting': <Lighting>,\n",
    "    'Spatial Layout': <Spatial Layout>,\n",
    "    'Architectural Features': <Architectural Features>}}]}\n",
    "\n",
    "Provided text : \"\"\"\n",
    "\n",
    "def normal_chat_completion(input_prompt, model='gpt-4-turbo-preview'):\n",
    "    client = openai.OpenAI()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": 'You are a smart and intelligent program that understands information and provides output in JSON format'},\n",
    "            {\"role\": \"user\", \"content\":input_prompt}\n",
    "        ]\n",
    "        )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = normal_chat_completion(output_formatting_prompt + outputs['living_15.jpg#living_16.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_step1 = dict()\n",
    "\n",
    "for f in tqdm(failed):\n",
    "    parsed = normal_chat_completion(output_formatting_prompt+outputs[f])\n",
    "    parse_step1[f] = parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_json(input_dict):\n",
    "    output_list = []\n",
    "    for item in input_dict['list']:\n",
    "        for key in item:\n",
    "            output_list.append(item[key])\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in parse_step1.items():\n",
    "    v = json.loads(v.choices[0].message.content)\n",
    "    outputs[k] = reformat_json(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['living_81.jpg#living_82.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_outputs = dict()\n",
    "\n",
    "for k,v in outputs.items():\n",
    "    k1, k2 = k.split(\"#\")\n",
    "    v1, v2 = v\n",
    "\n",
    "    indiv_outputs[k1] = v1\n",
    "    indiv_outputs[k2] = v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_outputs['living_11.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_outputs['living_1.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed = dict()\n",
    "\n",
    "for k,v in indiv_outputs.items():\n",
    "    tmp_dict = dict()\n",
    "    for kk, vv in v.items():\n",
    "        if isinstance(vv, list):\n",
    "            tmp_dict[kk] = ' '.join(vv)\n",
    "        else:\n",
    "            tmp_dict[kk] = vv\n",
    "    fixed[k] = tmp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed['living_1.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../data/room-dataset/room_descriptions_parsed.json\", 'w') as file:\n",
    "#     json.dump(fixed, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/room-dataset/room_descriptions_parsed.json\", 'r') as file:\n",
    "    final_outputs = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_outputs['living_1.jpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Yolo를 활용하여 가구 detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- YOLO class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import detect_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yolov5\n",
    "\n",
    "# 출처 : https://pypi.org/project/yolov5/\n",
    "\n",
    "# load pretrained model\n",
    "model = yolov5.load('yolov5s.pt')\n",
    "\n",
    "# set model parameters\n",
    "model.conf = 0.3  # NMS confidence threshold\n",
    "model.iou = 0.45  # NMS IoU threshold\n",
    "model.agnostic = False  # NMS class-agnostic\n",
    "model.multi_label = False  # NMS multiple labels per box\n",
    "model.max_det = 1000  # maximum number of detections per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = detect_objects('../data/room-dataset/living/living_18.jpg', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_furniture(detections):\n",
    "    furniture_class = [56, 57, 59, 60] # detections[0].names\n",
    "    furniture_names = ['chair', 'couch', 'bed', 'dining table']\n",
    "    furniture_detected = {}\n",
    "\n",
    "    filter = [True if (i in furniture_names) and (s>0.5) else False for i, s in zip(detections[1]['labels'], detections[1]['scores'])]\n",
    "    furniture_detected['boxes'] = detections[1]['boxes'][filter]\n",
    "    furniture_detected['scores'] = detections[1]['scores'][filter]\n",
    "    furniture_detected['categories'] = detections[1]['categories'][filter]\n",
    "    furniture_detected['labels'] = [item for item, bool in zip(detections[1]['labels'], filter) if bool==True]\n",
    "    \n",
    "    return furniture_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = filter_furniture(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2. 가구들을 crop + 좌표 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = dict()\n",
    "\n",
    "for img in tqdm(img_paths):\n",
    "    detect = detect_objects(img, model)\n",
    "    detections_parsed = filter_furniture(detect)\n",
    "    detections[img] = detections_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections['../data/room-dataset/living/living_10.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_bbox(pil_image, bbox):\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    crop_box = (x_min, y_min, x_max, y_max)\n",
    "\n",
    "    cropped_image = pil_image.crop(crop_box)\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "def normalize_image(pil_image, target_size=(224, 224)):\n",
    "    # resizing\n",
    "    resized_image = pil_image.resize(target_size, Image.LANCZOS)\n",
    "\n",
    "    # normalization\n",
    "    np_image = np.array(resized_image).astype('float32')\n",
    "    np_image /= 255.0  # pixel values to [0, 1]\n",
    "    normalized_image = Image.fromarray((np_image * 255).astype('uint8'))\n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_extract_coords(detection_results, base_path='../data/room-dataset/living_cropped/'):\n",
    "    for image_path, details in detection_results.items():\n",
    "        pil_image = Image.open(image_path)\n",
    "        if pil_image.mode == 'RGBA':\n",
    "            # Convert the image to RGB\n",
    "            # process 표준화 및 정확도 향상을 위해 변경\n",
    "            pil_image = pil_image.convert('RGB')\n",
    "\n",
    "        for i, bbox in enumerate(details['boxes']):\n",
    "            # Crop and normalize the image\n",
    "            cropped_image = crop_bbox(pil_image, bbox)\n",
    "            normalized_image = normalize_image(cropped_image, target_size=(112, 112))\n",
    "\n",
    "            # Save the normalized image\n",
    "            save_path = base_path + image_path.split('/')[-1].split('.')[0] + \"_\" + str(i) + \".jpg\"\n",
    "            normalized_image.save(save_path)\n",
    "\n",
    "    return detection_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_parsed = crop_and_extract_coords(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_parsed['../data/room-dataset/living/living_1.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_np_to_lists(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_np_to_lists(v) for k, v in obj.items()}\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_parsed = convert_np_to_lists(detections_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../data/room-dataset/room_detections_parsed.json\", 'w') as file:\n",
    "#     json.dump(detections_parsed, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
