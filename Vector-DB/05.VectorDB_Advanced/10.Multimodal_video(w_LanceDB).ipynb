{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GVx1H2iPsaeb"
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from pathlib import Path\n",
    "import speech_recognition as sr\n",
    "from pytube import YouTube\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "L12mtRSLtpkb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "X5iTUknBtqkT"
   },
   "outputs": [],
   "source": [
    "video_url = \"https://www.youtube.com/watch?v=d_qvLDhkg00\"\n",
    "output_video_path = \"../data/video_data/\"\n",
    "output_folder = \"../data/mixed_data/\"\n",
    "output_audio_path = \"../data/mixed_data/output_audio.wav\"\n",
    "\n",
    "filepath = output_video_path + \"input_vid.mp4\"\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "H57bFaHFtugw"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#추후 image retrieval 결과 플라팅 용\n",
    "def plot_images(image_paths):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for img_path in image_paths:\n",
    "        if os.path.isfile(img_path):\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            plt.subplot(2, 3, images_shown + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            images_shown += 1\n",
    "            if images_shown >= 7:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xLX2-Pxytx6s"
   },
   "outputs": [],
   "source": [
    "def download_video(url, output_path):\n",
    "    '''\n",
    "    url로부터 비디오 다운로드 후 output_path에 저장\n",
    "    '''\n",
    "    yt = YouTube(url)\n",
    "    metadata = {\"Author\": yt.author, \"Title\": yt.title, \"Views\": yt.views}\n",
    "    yt.streams.get_highest_resolution().download(\n",
    "        output_path=output_path, filename=\"input_vid.mp4\"\n",
    "    )\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def video_to_images(video_path, output_folder):\n",
    "    '''\n",
    "    video를 image로 분할\n",
    "    '''\n",
    "    clip = VideoFileClip(video_path)\n",
    "    clip.write_images_sequence(\n",
    "        os.path.join(output_folder, \"frame%04d.png\"), fps=0.2\n",
    "    )\n",
    "\n",
    "\n",
    "def video_to_audio(video_path, output_audio_path):\n",
    "    '''\n",
    "    video 내의 audio를 따로 추출\n",
    "    '''\n",
    "    clip = VideoFileClip(video_path)\n",
    "    audio = clip.audio\n",
    "    audio.write_audiofile(output_audio_path)\n",
    "\n",
    "\n",
    "def audio_to_text(audio_path):\n",
    "    '''\n",
    "    audio 파일을 STT하여 text파일로 저장\n",
    "    '''\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio = sr.AudioFile(audio_path)\n",
    "\n",
    "    with audio as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "        try:\n",
    "            text = recognizer.recognize_whisper(audio_data)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Speech recognition could not understand the audio.\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from service; {e}\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QNJ5yIa-t45j"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    metadata_vid = download_video(video_url, output_video_path)\n",
    "    video_to_images(filepath, output_folder)\n",
    "    video_to_audio(filepath, output_audio_path)\n",
    "    text_data = audio_to_text(output_audio_path)\n",
    "\n",
    "    with open(output_folder + \"output_text.txt\", \"w\") as file:\n",
    "        file.write(text_data)\n",
    "    print(\"Text data saved to file\")\n",
    "    file.close()\n",
    "    os.remove(output_audio_path)\n",
    "    print(\"Audio file removed\")\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vxxnFpEvt6Sv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [01:21<00:00, 4.36MiB/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "\n",
    "text_store = LanceDBVectorStore(uri=\"../data/lancedb\", table_name=\"text_collection\")\n",
    "image_store = LanceDBVectorStore(uri=\"../data/lancedb\", table_name=\"image_collection\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=text_store, image_store=image_store\n",
    ")\n",
    "\n",
    "# -> OpenAI CLIP 모델로 Text, Image Embedding 가해짐\n",
    "documents = SimpleDirectoryReader(output_folder).load_data()\n",
    "\n",
    "index = MultiModalVectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZCa_Xh6Pvlzf"
   },
   "outputs": [],
   "source": [
    "retriever_engine = index.as_retriever(\n",
    "    similarity_top_k=5, image_similarity_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GP_aJw7Av-1e"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metadata_str = json.dumps(metadata_vid)\n",
    "\n",
    "qa_tmpl_str = (\n",
    "    \"Given the provided information, including relevant images and retrieved context from the video, \\\n",
    " accurately and precisely answer the query without any additional prior knowledge.\\n\"\n",
    "    \"Please ensure honesty and responsibility, refraining from any racist or sexist remarks.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Context: {context_str}\\n\"\n",
    "    \"Metadata for video: {metadata_str} \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wJOWM6wdwh46"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.schema import ImageNode\n",
    "\n",
    "def retrieve(retriever_engine, query_str):\n",
    "    retrieval_results = retriever_engine.retrieve(query_str)\n",
    "\n",
    "    retrieved_image = []\n",
    "    retrieved_text = []\n",
    "    for res_node in retrieval_results:\n",
    "        if isinstance(res_node.node, ImageNode):\n",
    "            retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
    "        else:\n",
    "            display_source_node(res_node, source_length=200)\n",
    "            retrieved_text.append(res_node.text)\n",
    "\n",
    "    return retrieved_image, retrieved_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0rC0OnKwmz1"
   },
   "outputs": [],
   "source": [
    "query_str = \"Using examples from video, explain all things covered in the video regarding the gaussian function\"\n",
    "\n",
    "img, txt = retrieve(retriever_engine=retriever_engine, query_str=query_str)\n",
    "image_documents = SimpleDirectoryReader(\n",
    "    input_dir=output_folder, input_files=img\n",
    ").load_data()\n",
    "context_str = \"\".join(txt)\n",
    "plot_images(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6NoQ2diwrtz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "\n",
    "# text와 image를 모두 사용해 RAG를 해야하기 때문에 gpt-4-v 사용\n",
    "openai_mm_llm = OpenAIMultiModal(\n",
    "    model=\"gpt-4-vision-preview\", max_new_tokens=1500, api_key=openai_api_key,\n",
    ")\n",
    "\n",
    "response_1 = openai_mm_llm.complete(\n",
    "    prompt=qa_tmpl_str.format(\n",
    "        context_str=context_str, query_str=query_str, metadata_str=metadata_str\n",
    "    ),\n",
    "    image_documents=image_documents,\n",
    ")\n",
    "\n",
    "pprint(response_1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDnH8Herw3gM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN3bZi1yTs+DOb6UXzRy2bS",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
