{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding을 활용한 유사도 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec을 이용한 단어 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT야, simpsons 캐릭터 이름이 들어간 랜덤 문장 10개를 생성해줘\n",
    "\n",
    "sentences = [\"Homer Simpson forgot his lunch at home, so he had to buy a burger on his way to work.\",\n",
    "    \"Marge was busy knitting a new sweater for Bart's upcoming school play.\",\n",
    "    \"Lisa Simpson played a beautiful saxophone solo at the school concert.\",\n",
    "    \"Mr. Burns secretly plotted another scheme from his office at the Springfield Nuclear Power Plant.\",\n",
    "    \"Ned Flanders offered to help Homer fix the fence between their houses.\",\n",
    "    \"Bart Simpson tried a new prank at school, but it didn't go as planned.\",\n",
    "    \"Milhouse and Bart spent the afternoon playing video games and forgot to do their homework.\",\n",
    "    \"Maggie Simpson's adorable giggle filled the room as she played with her toys.\",\n",
    "    \"Apu had a busy day at the Kwik-E-Mart, dealing with a rush of customers.\",\n",
    "    \"Krusty the Clown decided to change his show a bit to attract a new audience.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# get rid of stopwords, lower case\n",
    "\n",
    "sentences = [re.sub(r\"[.',]\", \"\", sentence).lower().split(\" \") for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['homer',\n",
       " 'simpson',\n",
       " 'forgot',\n",
       " 'his',\n",
       " 'lunch',\n",
       " 'at',\n",
       " 'home',\n",
       " 'so',\n",
       " 'he',\n",
       " 'had',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'burger',\n",
       " 'on',\n",
       " 'his',\n",
       " 'way',\n",
       " 'to',\n",
       " 'work']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec\n",
    "\n",
    "skip_gram = Word2Vec(sentences, vector_size=300, min_count=1, window=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homer 의 vector representation : \n",
      "[-1.66068680e-03 -4.20303462e-04  1.09255337e-03 -2.18484551e-03\n",
      " -3.23499087e-03 -3.12648294e-03  3.04949097e-03  1.87829603e-03\n",
      " -1.61115616e-03 -2.80928891e-03  4.52324020e-04  9.58630932e-04\n",
      " -4.38321120e-04  4.20039461e-04 -1.44900673e-03  1.57583959e-03\n",
      "  5.10560756e-04  2.96067167e-03 -3.31212161e-03 -1.82445510e-03\n",
      " -3.01285670e-03 -1.11729649e-04 -2.61634029e-03  1.71959959e-03\n",
      " -2.11100094e-03 -2.03485345e-03  1.69092277e-03 -2.72677210e-03\n",
      "  4.76851710e-04 -2.43410910e-03  3.28659941e-03  2.89322482e-03\n",
      "  5.95899648e-04  1.94471062e-03  1.52419391e-03 -1.98301394e-03\n",
      "  3.28957522e-03 -3.26667982e-03  2.68764189e-03  9.17330268e-04\n",
      " -9.88418004e-04 -1.19753159e-03  3.04961996e-03 -1.81994250e-03\n",
      "  2.79433304e-03 -1.95553945e-03  2.79207341e-03 -1.48160951e-04\n",
      "  2.66201468e-03 -1.02206389e-03  2.01950571e-03  2.95562134e-03\n",
      "  8.11300066e-04  4.55215428e-04  1.65783032e-03  2.71425396e-03\n",
      "  2.85594515e-03  2.84643611e-03  2.36571440e-03  2.69569410e-03\n",
      "  2.86536408e-03  1.06979742e-05 -3.44244880e-04  5.48140437e-04\n",
      " -5.46397168e-05  2.81422050e-04 -2.87100067e-03 -3.18586407e-03\n",
      " -7.79178401e-04  2.93133804e-03 -1.23406888e-03 -2.29325425e-03\n",
      "  1.63925881e-03  3.19730287e-04  6.20175560e-04  1.22176390e-03\n",
      "  1.14811910e-03  1.92712888e-03  4.05051280e-04  3.37304111e-04\n",
      "  3.05107539e-03  9.27790359e-04 -1.60060322e-03  2.26521911e-03\n",
      "  1.76511216e-03  9.88276559e-04 -1.08301686e-03  1.13988691e-03\n",
      "  2.16252939e-03  2.34275823e-03  3.24678927e-04 -2.81540281e-03\n",
      "  7.21826509e-05  1.40061165e-04  1.34546531e-03 -3.10492166e-03\n",
      "  3.28488904e-03 -2.38402886e-03  1.96512206e-03 -3.11573036e-03\n",
      "  3.24020907e-03  2.45439238e-03  4.50846332e-04 -1.12356886e-03\n",
      " -1.32229543e-04  1.22046615e-04 -2.09888932e-03  1.93005183e-03\n",
      "  7.24676589e-04  1.26012217e-03 -2.42453138e-03  2.82842526e-03\n",
      "  1.41696830e-04 -8.03092407e-05 -3.03274323e-03  1.35573710e-03\n",
      "  2.25264905e-03  2.45702779e-03 -2.15250649e-03 -2.68144207e-03\n",
      " -1.84272008e-03 -1.78062328e-04 -2.73930421e-03 -2.76312162e-03\n",
      " -6.49262918e-04  3.79283010e-04 -3.19513585e-03 -1.26098550e-03\n",
      "  2.15646302e-04  2.32684077e-03  5.77375351e-04 -1.92653519e-04\n",
      " -2.47276342e-03 -2.28028302e-03 -2.15097447e-04  2.50997022e-03\n",
      "  1.80709839e-03 -5.11256221e-04  3.89818801e-04 -3.23218526e-03\n",
      " -4.55559610e-04 -1.58451556e-03  1.93722581e-03 -7.54477456e-04\n",
      " -1.55739812e-03 -3.19571234e-03 -4.31633816e-04 -2.40551936e-03\n",
      " -5.34374383e-04 -1.37561746e-03 -7.84670701e-04 -1.12861244e-03\n",
      " -2.72948877e-03 -4.06166451e-04  5.73715777e-04 -1.33450981e-03\n",
      " -2.56841327e-03 -1.21285731e-03 -3.02833086e-03 -1.85324418e-04\n",
      "  1.97609677e-03 -9.44438973e-04  1.04032643e-03  1.68447522e-03\n",
      "  2.78418534e-03  1.86675217e-03  3.20062647e-03 -3.20460391e-03\n",
      " -2.68438552e-03 -2.21391930e-03 -2.53441045e-03 -2.67933286e-03\n",
      " -2.59062601e-03 -9.35797405e-04  4.70210012e-04 -9.60970414e-04\n",
      " -2.90283281e-03  1.63133163e-03  3.08230374e-04  1.53027149e-03\n",
      "  2.39912583e-03  2.56149494e-03 -2.97559076e-04  1.21046661e-03\n",
      " -1.73652580e-03  6.26456167e-04  1.51195272e-03  3.34489834e-03\n",
      " -1.06545002e-03  9.09453316e-04 -1.90982758e-03 -7.30646891e-04\n",
      "  2.71163764e-03 -1.30748888e-03 -3.96405841e-04 -3.12943314e-03\n",
      " -3.15011223e-03  2.96484376e-03 -1.89172395e-03  1.65633566e-03\n",
      " -2.33992934e-03 -8.17773107e-04 -2.67595844e-03  2.52207834e-03\n",
      "  2.07958114e-03  1.73536746e-03  2.80222762e-03 -2.07082237e-04\n",
      " -3.09491903e-03  3.03551811e-03 -1.69026875e-03  2.57623312e-03\n",
      "  1.81319134e-03 -3.93923052e-04 -2.51527084e-03 -5.22290589e-04\n",
      "  2.01459555e-03 -2.37731100e-03  4.18180018e-04 -2.67173676e-03\n",
      "  2.90240673e-03 -9.63921542e-04  3.13616078e-03 -1.91106240e-03\n",
      " -3.27868620e-03 -2.89353030e-03 -1.34260673e-03  1.56894093e-03\n",
      " -6.50069196e-05  3.04524880e-03  1.07473449e-03  1.23249064e-03\n",
      "  1.00344489e-03  2.73522572e-03 -8.72792734e-04  2.49517243e-03\n",
      " -3.16875358e-03  9.50374117e-04 -2.13087042e-04  8.35450919e-05\n",
      "  2.30753538e-03 -9.51145252e-04 -8.14512488e-04  1.94224140e-05\n",
      " -1.65353616e-04 -1.20843516e-03  2.11570039e-03 -2.22024950e-03\n",
      "  2.63734371e-03 -1.30435174e-05  8.47343297e-04  1.09062018e-03\n",
      " -7.62929703e-05  5.49671764e-04 -1.08596124e-03  1.55842700e-03\n",
      "  9.04466433e-05 -1.12329691e-03 -2.93511851e-03 -3.35479411e-03\n",
      "  1.13948714e-04 -1.90499390e-03 -3.52673727e-04 -1.46952574e-03\n",
      " -2.92333635e-03  3.45982437e-04  2.00314121e-03 -7.07993808e-04\n",
      " -2.44983658e-03  1.04414253e-03 -1.54392284e-04 -1.81977404e-03\n",
      " -3.66952881e-04 -1.86574136e-04 -1.04066113e-03 -3.27943615e-03\n",
      "  2.55812518e-03  1.24969310e-03 -8.64422240e-04  2.39443034e-03\n",
      "  1.55546571e-04  2.37179967e-03 -5.29629004e-04  2.50017340e-03\n",
      " -1.58586972e-05 -1.98490988e-03 -1.60756963e-03  3.23407492e-03\n",
      "  1.77424648e-04  2.97734747e-04  2.80875596e-03 -2.06154329e-03\n",
      " -5.77152066e-04 -2.73376517e-03 -2.23413017e-03 -2.84535927e-03\n",
      "  1.31321000e-03  9.17847501e-04  1.90934294e-03  8.00744805e-04]\n"
     ]
    }
   ],
   "source": [
    "print(\"{} 의 vector representation : \\n{}\".format('homer', skip_gram.wv.get_vector(skip_gram.wv.key_to_index['homer'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marge', 0.14081521332263947),\n",
       " ('offered', 0.13243569433689117),\n",
       " ('games', 0.12250109761953354),\n",
       " ('her', 0.11486156284809113),\n",
       " ('nuclear', 0.10569246113300323),\n",
       " ('do', 0.09913020581007004),\n",
       " ('toys', 0.0984482690691948),\n",
       " ('office', 0.09244135022163391),\n",
       " ('bart', 0.09009940177202225),\n",
       " ('way', 0.08802291750907898)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram.wv.most_similar(\"homer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직접 유사도 구해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "homer_vector = skip_gram.wv.get_vector(skip_gram.wv.key_to_index['homer'])\n",
    "marge_vector = skip_gram.wv.get_vector(skip_gram.wv.key_to_index['marge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 계산하기 from scratch\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vector_a: np.ndarray, vector_b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    두 벡터간 cosine similarity를 계산\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vector_a : np.ndarray\n",
    "        The first input vector.\n",
    "    vector_b : np.ndarray\n",
    "        The second input vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The cosine similarity between `vector_a` and `vector_b`, which is a value between -1 and 1.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dot_product = np.dot(vector_a, vector_b)\n",
    "    norm_a = norm(vector_a)\n",
    "    norm_b = norm(vector_b)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14081518"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(homer_vector, marge_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpsons dataset을 활용한 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sckim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sckim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158314, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/simpsons_dataset.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_character_text                                       spoken_words\n",
       "0              Miss Hoover  No, actually, it was a little of both. Sometim...\n",
       "1             Lisa Simpson                             Where's Mr. Bergstrom?\n",
       "2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n",
       "3             Lisa Simpson                         That life is worth living.\n",
       "4  Edna Krabappel-Flanders  The polls will be open from now until the end ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_character_text    17814\n",
       "spoken_words          26459\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No, actually, it was a little of both. Sometimes when a disease is in all the magazines and all the news shows, it's only natural that you think you have it.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'spoken_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize and remove the stopwords and non-alphabetic characters for each line of dialogue\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "\n",
    "def cleaning(doc):\n",
    "    \"\"\"\n",
    "    Cleans a spaCy Doc object by lemmatizing its tokens and removing stop words,\n",
    "    then joins the remaining tokens into a single string if there are more than two tokens left.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    doc : spacy.tokens.Doc\n",
    "        A spaCy Doc object containing the processed text.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    Optional : str\n",
    "        A string composed of the lemmatized, non-stop tokens separated by spaces,\n",
    "        if the resulting list of tokens has more than two elements. Otherwise, returns None.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep alphabets\n",
    "cleaner = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['spoken_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [cleaning(doc) for doc in nlp.pipe(cleaner, batch_size=5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'actually little disease magazine news show natural think'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85956, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe에 넣어서 null이 있는 대화는 삭제\n",
    "# 주로 null은 특정 행동을 했지만 대화가 없었을 때임\n",
    "\n",
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 문장을 여러 단위의 단어로 분할\n",
    "sentences = [s.split(' ') for s in df_clean['clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85956"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actually',\n",
       " 'little',\n",
       " 'disease',\n",
       " 'magazine',\n",
       " 'news',\n",
       " 'show',\n",
       " 'natural',\n",
       " 'think']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `window` : 문장 내에서 현재 단어와 예측 단어 사이의 최대 거리. ex) 타겟 단어의 왼쪽과 오른쪽 n번째 단어\n",
    "- `vector_size` : 단어 벡터의 차원 수\n",
    "- `min_count` : 이 값보다 총 절대 빈도수가 낮은 모든 단어를 무시함 - (2, 100)\n",
    "- `sg` : 1은 skip-gram, 0은 CBOW method를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 하기\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장에 들어있는 각 단어들을 Word2Vec 모델이 인식할 수 있는 형태로 변환\n",
    "w2v_model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19983507, 54001900)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(w2v_model.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어간 유사도 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(w2v_model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- most_similar : 주어진 조건에 가장 적합한 단어 탐색\n",
    "- similarity : 주어진 단어들의 유사도 계산\n",
    "- doesnt_match : 주어진 단어들 중 가장 '덜 유사한' 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(w2v_model.wv.most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(w2v_model.wv.similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marge', 0.4234519898891449),\n",
       " ('bart', 0.3308035433292389),\n",
       " ('simpson', 0.31797534227371216),\n",
       " ('mr', 0.2797403931617737),\n",
       " ('lisa', 0.2631458640098572),\n",
       " ('barney', 0.25580036640167236),\n",
       " ('son', 0.24946914613246918),\n",
       " ('husband', 0.24716949462890625),\n",
       " ('wife', 0.24083773791790009),\n",
       " ('mrs', 0.23470626771450043)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"homer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.41932833194732666),\n",
       " ('homer', 0.3308035135269165),\n",
       " ('dad', 0.32987260818481445),\n",
       " ('mom', 0.32922524213790894),\n",
       " ('milhouse', 0.3127206861972809),\n",
       " ('boy', 0.3014339506626129),\n",
       " ('kid', 0.28203287720680237),\n",
       " ('son', 0.2761775553226471),\n",
       " ('child', 0.27022725343704224),\n",
       " ('think', 0.26444777846336365)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"bart\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Woman : homer = ___ : marge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.20725250244140625),\n",
       " ('modern', 0.20201577246189117),\n",
       " ('see', 0.20097629725933075)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\", \"homer\"], negative=[\"marge\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.3182581067085266),\n",
       " ('maggie', 0.2833455801010132),\n",
       " ('mom', 0.2700842022895813)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\", \"bart\"], negative=[\"man\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bart'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['bart', 'homer', 'marge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marge'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['bart', 'lisa', 'marge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 임베딩의 한계점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_vector = w2v_model.wv.get_vector(w2v_model.wv.key_to_index['bank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우리가 사용하는 모든 단어는 context에 따라 의미가 다르다\n",
    "- 단어 embedding의 경우 이런 유연성을 확보하지 못 함\n",
    "    - 배를 깎아 먹었다 / 배가 고프다 / 배 멀미를 하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b266124324844e3982cd06a553fb37f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sckim\\.conda\\envs\\vectordb\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sckim\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a733bb3be544fb91c5136bc288457b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69838b3bc86a4d60a5b7b2e11b2e8ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e32cd3dc6744b238238194085a6ee60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267fd7abf44246a18340e60e1bbdaa3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pre-trained model tokenizer와 and bert model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # smaller & uncased model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank가 들어간 유사한 문장 두 개\n",
    "sentence1 = \"I deposited money at the bank.\"\n",
    "sentence2 = \"The ducks swam to the river bank.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 BERT가 인식할 수 있는 형태로 Tokenize\n",
    "encoded_input1 = tokenizer(sentence1, return_tensors='pt') # pytorch\n",
    "encoded_input2 = tokenizer(sentence2, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045, 14140,  2769,  2012,  1996,  2924,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1996, 14875, 16849,  2000,  1996,  2314,  2924,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `input_ids` : 각 단어별로 매핑된 key. 101은 문장의 시작을, 102는 문장의 끝을 의미\n",
    "- `token_type_ids` : 문장 번호\n",
    "- `attention_mask` : attention을 가져야 하는 단어는 1, 그렇지 않은 단어는 0. (만약 input이 실제 단어들이라면 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 생성!\n",
    "with torch.no_grad():\n",
    "    output1 = model(**encoded_input1)\n",
    "    output2 = model(**encoded_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 내에서 bank라는 단어 찾아오기 (문장의 5번째에 있는 단어)\n",
    "bank_embedding_sentence1 = output1.last_hidden_state[0, 5, :]\n",
    "bank_embedding_sentence2 = output2.last_hidden_state[0, 5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the two embeddings: 0.592241\n"
     ]
    }
   ],
   "source": [
    "# cosine similarity 계산\n",
    "\n",
    "similarity = cosine_similarity(bank_embedding_sentence1, bank_embedding_sentence2)\n",
    "# print(\"Embedding for 'bank' in sentence 1:\", bank_embedding_sentence1)\n",
    "# print(\"Embedding for 'bank' in sentence 2:\", bank_embedding_sentence2)\n",
    "print(\"Cosine similarity between the two embeddings:\", similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
